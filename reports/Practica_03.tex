\documentclass[a4paper, 11pt, titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{kvoptions-patch}
\usepackage[title={Práctica 3: Algoritmos genéticos}]{estilo}

\makeatletter
 \renewcommand{\ALG@name}{Pseudocódigo}
\makeatother

\pgfplotstableread[col sep=comma]{../results/03/knn.csv}\dataKNN
\pgfplotstableread[col sep=comma]{../results/03/SFS.csv}\dataSFS
\pgfplotstableread[col sep=comma]{../results/03/generationalGA.csv}\dataAGG
\pgfplotstableread[col sep=comma]{../results/03/stationaryGA.csv}\dataAGE
\pgfplotstableread[col sep=comma]{../results/03/HUXgenerationalGA.csv}\dataAGGHUX
\pgfplotstableread[col sep=comma]{../results/03/HUXstationaryGA.csv}\dataAGEHUX
\pgfplotstableread[col sep=comma]{../results/03/medias.csv}\dataMedias

\begin{document}

    \maketitle

    \pagenumbering{roman}
    \tableofcontents
    \newpage

    \pagenumbering{arabic}

    \section{Descripción del problema}
    La selección de características es una técnica muy usada en problemas de aprendizaje automático.

    El aprendizaje automático, visto de una forma muy general, tiene como objetivo clasificar un conjunto de objetos ---modelador por una serie de atributos--- en clases.

    Esta clasificación se aprende desde los datos, pero la selección de los atributos que definen la modelización del objeto puede no ser la más apropiada: en ocasiones hay atributos superfluos o demasiado ruidosos que sería conveniente eliminar. Además, cuantos menos atributos definan un objeto, más rápido y preciso será el aprendizaje. Es aquí entonces donde aparece la pregunta que guia todo este trabajo: ¿cómo identificar los atributos que mejor aprendizaje promueven?

    La respuesta a esta pregunta pasa por la selección de características, cuyo objetivo es reducir la definición de un objeto a una serie de características que faciliten el aprendizaje.

    La idea es entonces la siguiente: dado un conjunto de $m$ objetos definidos por un conjunto $C$ de $n$ características, y considerado un modelo de aprendizaje $f$ que intenta aprender la clasificación de estos objetos, encontrar el subconjunto $C' \subset C$ que maximiza el modelo $f$.

    Así, vemos claramente que el tamaño de caso de nuestro problema es $n$ ---el número de características--- y que el objetivo está bien definido: eliminar aquellas características que o bien empeoren la bondad de $f$ o bien sean innecesarias.

    Con todos estos elementos definidos, podemos pasar a analizar las metaheurísticas consideradas.

    \section{Metaheurísticas}

    \subsection{Introducción}

    Los algoritmos considerados para resolver el problema son los siguientes:
    \begin{itemize}
        \item Algoritmo genético generacional.
        \item Algoritmo genético estacionario.
    \end{itemize}

    Además, compararemos estas metaheurísticas con el algoritmo voraz \emph{Sequential forward selection}.

    Estas dos metaheurísticas reúnen las condiciones necesarias para resolver el problema: el espacio de soluciones de nuestro problema puede ser analizado mediante las estructuras de generación de vecinos y los criterios de aceptación que utilizan estos algoritmos. Veamos con un poco más de detalle los aspectos comunes a las metaheurísticas implementadas:

    \subsubsection*{Datos de entrada}
    Todos los algoritmos considerados reciben un conjunto de entrenamiento cuyos objetos tienen la siguiente estructura:
    \[
    (s_1, s_2, \dots, s_n, c)
    \]
    donde $(s_1, s_2, \dots, s_n)$ es el conjunto de valores de los atributos que definen el objeto y $c$ la clase a la que pertenece.

    \subsubsection*{Esquema de representación}
    El espacio de soluciones $S$ de nuestro problema es el conjunto de todos los vectores $s$ de longitud $n$ ---el número de características--- binarios; es decir:
    \[
    S = \{s = (s_1, s_2, \dots, s_n) / s_i \in \{0,1\} \;\forall i = 1, 2, \dots, n\}
    \]

    La posición $i$-ésima de un vector $s \in S$ indicará la inclusión o no de la característica $i$-ésima en el conjunto final $C'$.

    \subsubsection*{Función objetivo}
    La finalidad de las metaheurísticas será maximizar la función objetivo siguiente:
    \begin{align*}
        f \colon &S \to [0,100] \\
        &s \mapsto f(s) = \textrm{Acierto del 3-NN sobre s}
    \end{align*}

    $f(s)$ es, por tanto, la tasa de acierto del clasificador 3-NN producido a partir de la solución $s$.

    El clasificador 3-NN es una particularización del clasificador $k$-NN, que mide la distancia de la instancia considerada a todos los demás objetos en el conjunto de datos de entrenamiento y le asigna la clasificación mayoritaria de entre los $k$ vecinos más cercanos; esto es:

    \begin{algorithm}
        \caption{Clasificador $k$-NN}\label{knn}
        \begin{algorithmic}[1]
            \Function{$k$-NN}{instance, trainingData}
            \State distances $\gets$ euclideanDistance(instance, trainingData)
            \State neighbours $\gets$ getClosestNeighbours(distances)
            \State classification $\gets$ mostVotedClassification(neighbours)
            \State \Return classification
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Así, dada una solución $s \in S$, la función objetivo es como sigue:

    \begin{algorithm}
        \caption{Función objetivo}\label{f_objetivo}
        \begin{algorithmic}[1]
            \Function{$f$}{s, train, target}
            \State samples $\gets$ removeZeroColumns(s, train)
            \State sum $\gets$ 0

            \For{instance $\in$ samples}
                \State class $\gets$ k-NN(instance, samples)
                \State sum $\gets$ sum + \begin{cases}
                        1 &\textrm{\textbf{if} } \textrm{class} = \textrm{actualClass(instance, target)} \\
                        0 &\textrm{\textbf{if} } \textrm{class} \neq \textrm{actualClass(instance, target)}
                    \end{cases}
            \EndFor

            \State \Return sum / (number of samples in train)
            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    donde \emph{removeZeroColumns(s, train)} elimina la columna $i$-ésima de \emph{train} si y sólo si $s_i = 0$ y \emph{actualClass(instance, target)} devuelve la clase real ---no la aprendida--- del objeto \emph{instance}.


    \subsubsection*{Entorno de soluciones}
    Dada una solución $s \in S$, el entorno de soluciones vecinas a $s$ es el conjunto
    \[
    E(s) = \{s' \in S / \vert s' - s \vert = (0, \dots, 0, \underbrace{1}_i, 0, \dots, 0), i\in\{1,2, \dots, n\}\}
    \]
    es decir, $E(s)$ son las soluciones que difieren de $s$ en una única posición. Es evidente entonces que el conjunto $E(S)$ tiene siempre exactamente cardinal igual a $n$.

    El operador de generación de vecino de la solución $s$ es entonces como sigue:
    \begin{algorithm}
        \caption{Operador de generación de vecino}\label{flip}
        \begin{algorithmic}[1]
            \Function{flip}{solution, feature}
            \State $s' \gets solution$
            \State $s'[feature] \gets (s'[feature] + 1)$ mod 2
            \State \Return s'
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


    % TODO: Hablar de la función score y del leaveoneout y esas mierdas

    \subsubsection*{Criterios de parada}
    Aunque los criterios de parada dependerán de la metaheurística considerada ---en general se parará cuando no se encuentre mejora en el entorno---, en todos los algoritmos pararemos necesariamente tras llegar a las 15000 evaluaciones con el clasificador 3-NN sobre las soluciones generadas.

    \subsubsection*{Generación de soluciones aleatorias}

    En los algoritmos de búsqueda multiarranque básica y búsqueda local reiterada se genera una serie de soluciones aleatorias sobre las que se aplica búsqueda local de una u otra forma. La generación de estas soluciones aleatorias sigue el siguiente esquema:

    \begin{algorithm}
        \caption{Generación de soluciones aleatorias}\label{randomSol}
        \begin{algorithmic}[1]
            \Function{randomSolution}{size}
            \For{$i \in 1,2,\dots,size$}
                \State random $\gets$ uniformRandomNumber([0,1])
                \State $s_i$ $\gets$ \begin{cases}
                    0 &$\textrm{\textbf{if} }$ random \leq 0.5 \\
                    1 &$\textrm{\textbf{if} }$ random  > 0.5
                \end{cases}
            \EndFor
            \State solution $\gets$ $(s_1, s_2, \dots, s_{size})$
            \State \Return solution
        \end{algorithmic}
    \end{algorithm}


    \subsubsection*{Mecanismo de selección}

    En ambos algoritmos se considera un mecanismo de selección basado en el torneo binario; es decir, se eligen dos individuos de la población al azar y se selecciona el mejor. En el Pseudocódigo \ref{torneoBinario} se puede ver este procedimiento:

    \begin{algorithm}
        \caption{Torneo binario}\label{torneoBinario}
        \begin{algorithmic}[1]
            \Function{binaryTournament}{population}
            \State $contestants$ $\gets$ randomly pick 2 chromosomes from population
            \State winner $\gets$ best($contestants_1$, $contestantes_2$)
            \State \Return winner
        \end{algorithmic}
    \end{algorithm}

    Un mecanismo de selección genérico, que sirve para ambos algoritmos genéticos considerados ---ajustando simplemente el número de individuos seleccionados---, puede verse en el Pseudocódigo \ref{seleccion}.

    \begin{algorithm}
        \caption{Mecanismo de selección}\label{seleccion}
        \begin{algorithmic}[1]
            \Function{selection}{population, numSelected}
            \For{$i \in \{1,2,\dots,numSelected\}$}
                \State $s_i \gets$ binaryTournament(population)
            \EndFor
            \State selected $\gets$ $(s_1, s_2, \dots, s_{numSelected})$
            \State \Return selected
        \end{algorithmic}
    \end{algorithm}

    \subsubsection*{Operador de cruce}

    En esta práctica se han considerados dos operadores de cruces: el clásico en dos puntos y el uniforme.

    El operador de cruce clásico en dos puntos consiste en lo siguiente: dividir los dos padres en tres partes iguales para ambos individuos pero de tamaño aleatorio y asignar a cada hijo toma la parte central de un padre y las partes exteriores del otro. Podemos ver este operador en el Pseudocódigo \ref{opCruceClasico}.

    \begin{algorithm}
        \caption{Operador de cruce clásico}\label{opCruceClasico}
        \begin{algorithmic}[1]
            \Function{classicXOver}{f, m} \Comment{Father and mother}
            \State $i, j \gets$ pick 2 random integers in $\{2,\dots,n-1\}$ \Comment{n = number of genes}
            \State $c^1 \gets (f_1, f_2, \dots, f_i, m_{i+1}, m_{i+2}, \dots, m_j, f_{j+1}, f_{j+2}, \dots, f_n)$
            \State $c^2 \gets (m_1, m_2, \dots, m_i, f_{i+1}, f_{i+2}, \dots, f_j, m_{j+1}, m_{j+2}, \dots, m_n)$
            \State children $\gets$ [$c^1, c^2$]
            \State \Return children
        \end{algorithmic}
    \end{algorithm}

    Además, se han hecho las mismas pruebas con el operador uniforme, consistente en heredar de los padres aquellos genes que ambos progenitores tienen iguales y tomar uno aleatorio en aquellos en los que difieren. Podemos ver este operador en el Pseudocódigo \ref{opCruceHUX}.

    \begin{algorithm}
        \caption{Operador de cruce uniforme}\label{opCruceHUX}
        \begin{algorithmic}[1]
            \Function{uniformXOver}{f, m} \Comment{Father and mother}
            \For{$i \in \{1,2,\dots,n\}$} \Comment{n = number of genes}
                \For{$j \in \{1,2\}$}
                    \State $c^j_i \gets$ \begin{cases}
                        f_i &\textrm{\textbf{if} } f_i = m_i \\
                        random(\{0,1\}) &\textrm{\textbf{if} } f_i \neq m_i
                \end{cases}
                \EndFor
            \EndFor
            \State children $\gets$ [$c^1, c^2$]
            \State \Return children
        \end{algorithmic}
    \end{algorithm}

    \subsubsection*{Operador de mutación}

    Por último, estudiemos el operador de mutación considerado, dependiente de un proceso también aleatorio. La idea inicial era la siguiente: para cada gen de cada cromosoma se genera un número aleatorio; si este es menor que una constante $\alpha$, se muta el gen; si no, se deja tal y como está.

    Como la constante considerada es ínfima ---$\alpha = 0.001$---, el coste computacional de generar un número aleatorio para cada gen de cada individuo de la población es muy alto: generaremos demasiados valores aleatorios para las pocas mutaciones que vamos a realizar. Por tanto, se ha seguido un procedimiento basado en el número esperado de mutaciones; es decir: se calcula el número $M = \alpha n N$, donde $\alpha$ es la probabilidad de mutación, $n$ el número de genes en un cromosoma y $N$ el número de cromosomas en la población y se eligen $M$ genes de entre todos los cromosomas a los que se le aplica la mutación. El operador de mutación atómico ---esto es, el procedimiento que se le aplica a cada gen si se decide mutarlo--- es el operador \emph{flip}. En el Pseudocódigo \ref{opMutacion} se puede ver todo este proceso con más detalle, donde se indica que los números aleatorios generados en las $M$ iteraciones deben ser siempre distintos para no mutar un mismo cromosoma con un mismo gen dos veces.

    \begin{algorithm}
        \caption{Operador de mutación}\label{opMutacion}
        \begin{algorithmic}[1]
            \Function{mutate}{population}
            \State $\alpha \gets 0.001$
            \State $n \gets$ size of a chromosome
            \State $N \gets$ number of chromosomes in population
            \State $M \gets \ceil{\alpha n N}$
            \For{$\_ \in \{1,2,\dots,M\}$} \Comment{Repeat it M times}
                \State\Comment{Do not repeat the pair \7{chromosome, gene\7} between iterations}
                \State chromosome $\gets random(\{1,\dots,N\})$
                \State gene $\gets random(\{1,\dots,n\})$
                \State flip(chrom, gene)
            \EndFor
            \State \Return population
        \end{algorithmic}
    \end{algorithm}

    \subsubsection*{Esquema de evolución}

    Aunque el esquema de evolución es distinto en las variantes generacional y estacionaria de los dos algoritmos genéticos, se ha generalizado el procedimiento para poder usarlo con ambos.

    Las únicas dependencias que tiene con el modelo es el número de seleccionados y el operador de cruce, así que las tomaremos como entrada de un procedimiento para poder describir el método de forma general.

    La idea de la evolución es la siguiente: para cada pareja de individuos seleccionados en $P_t$ se generará un número aleatorio; si este es menor que una constante $\alpha$ prefijada, a esta pareja se le aplicará el operador de cruce, guardando el resultado como parte de la población de descendientes; si el número aleatorio generado es mayor que $\alpha$, la pareja de individuos seleccionados pasará a formar parte de la población de descendientes sin más.

    Veamos el procedimiento general en el Pseudocódigo \ref{recombination}

    \begin{algorithm}
        \caption{Esquema de evolución}\label{recombination}
        \begin{algorithmic}[1]
            \Function{recombination}{selected, $\alpha$} \Comment{Selected chromosomes in $P_t$}
            \State $D \gets \o$ \Comment{Descendants of $P_t$}
            \For{f,m $\in$ pairs(selected)} \Comment{Take a different pair in each iteration}
                \State random $\gets$ random([0.0,1.0])
                \State $D \gets D\; \cup$ \begin{cases}
                    crossover(f,m) &\textrm{\textbf{if} } random < \alpha \\
                    \{f,m\} &\textrm{\textbf{if} } random \geq \alpha
                \end{cases}
            \EndFor
            \State \Return $D$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


    \subsection{Algoritmo genético generacional}

    Con todos los procedimientos ya presentados, sólo falta por describir el esquema de reemplazamiento. En esta primer versión del algoritmo genético, la población $P_{t+1}$ está formada por todos los descendientes de $P_t$ ---esto es, el conjunto de individuos devuelto por el método \emph{recombination}---, asegurando el elitismo; es decir, que la mejor solución de $P_t$ esté en $P_{t+1}$. El procedimiento, que recibe como parámetros la población $P_t$ y los descendientes $D$ generados con el esquema de evolución recién descrito, puede verse en el Pseudocódigo \ref{replacementG}.

    \begin{algorithm}
        \caption{Reemplazamiento generacional}\label{replacementG}
        \begin{algorithmic}[1]
            \Function{genReplacement}{$P_t$, $D$} \Comment{Actual population and its descendants}
            \State $M \gets$ best chromosome in $P_t$
            \State $P_{t+1} \gets D$
            \If{$M \notin P_{t+1}$}
                \State $W \gets$ worst chromosome in $P_{t+1}$
                \State $P_{t+1} \gets (P_{t+1} \setminus \{W\}) \cup \{M\}$
            \EndIf
            \State \Return $P_{t+1}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Con todo esto estamos en disposición ya de ver el comportamiento del algoritmo genético generacional al completo. En el Pseudocódigo \ref{AGG} se encuentra resumido el procedimiento, cuyo criterio de parada depende del número de llamadas a la función objetivo: cuando se alcancen 15000, se detendrá el proceso y se devolverá la mejor solución.

    \begin{algorithm}
        \caption{Algoritmo genético generacional}\label{AGG}
        \begin{algorithmic}[1]
            \Function{AGG}{}
            \State $\alpha \gets 0.7$ \Comment{Crossover probability}
            \State $N \gets 30$ \Comment{Size of the population}
            \State $P_t \gets$ generate $N$ random chromosomes
            \While{calls to the target function < 15000}
                \State $S \gets$ selection($P_t$, $N$) \Comment{Selection of $N$ parents}
                \State $D \gets$ recombination($S$, $\alpha$)
                \State $D' \gets$ mutation($D$)
                \State $P_{t+1} \gets$ genReplacement($P_t$, $D'$)
                \State $P_t \gets P_{t+1}$
            \EndWhile
            \State bestSolution $\gets$ bestChromosome($P_t$)
            \State bestScore $\gets$ $f$(bestSolution)
            \State \Return bestChromosome, bestScore
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


    \subsection{Algoritmo genético estacionario}

    En esta segunda versión del algoritmo genético, el esquema de reemplazamiento es diferente, ya que la población $P_{t+1}$ surge de la $P_t$ haciendo un único cambio: tras generar un par de descendientes de un par de seleccionados de $P_t$ por torneo binario, estos hijos compiten por entrar en $P_{t+1}$, sustituyendo en caso de victoria a las dos peores soluciones de $P_t$. El procedimiento, que recibe como parámetros la población $P_t$ y los descendientes $D$ generados con el esquema de evolución recién descrito, puede verse en el Pseudocódigo \ref{replacementG}.

    \begin{algorithm}
        \caption{Reemplazamiento estacionario}\label{replacementG}
        \begin{algorithmic}[1]
            \Function{statReplacement}{$P_t$, $D$} \Comment{There are only two descendants}
            \State $w_1 \gets$ worst chromosome in $P_t$
            \State $w_2 \gets$ worst chromosome in $P_t \setminus \{w_1\}$
            \State $P_{t+1} \gets P_t$
            \State $d_2 \gets$ best chromosome in $D$
            \State $d_1 \gets$ best chromosome in $D \setminus d_2$
            \For{$i \in \{1,2\}$}
                \If{$d_i$ is better than $w_i$}
                    \State $P_{t+1} \gets (P_{t+1} \setminus \{w_i\}) \cup \{d_i\}$
                \EndIf
            \EndFor
            \State \Return $P_{t+1}$
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    Con todo esto estamos en disposición ya de ver el comportamiento del algoritmo genético estacionario al completo. En el Pseudocódigo \ref{AGE} se encuentra resumido el procedimiento, cuyo criterio de parada es, de nuevo, llegar a 15000 llamadas a la función objetivo.

    \begin{algorithm}
        \caption{Algoritmo genético estacionario}\label{AGE}
        \begin{algorithmic}[1]
            \Function{AGE}{}
            \State $\alpha \gets 1.0$ \Comment{Crossover probability}
            \State $N \gets 30$ \Comment{Size of the population}
            \State $P_t \gets$ generate $N$ random chromosomes
            \While{calls to the target function < 15000}
                \State $S \gets$ selection($P_t$, 2) \Comment{Selection of two random parents}
                \State $D \gets$ recombination($S$, $\alpha$)
                \State $D' \gets$ mutation($D$)
                \State $P_{t+1} \gets$ statReplacement($P_t$, $D'$)
                \State $P_t \gets P_{t+1}$
            \EndWhile
            \State bestSolution $\gets$ bestChromosome($P_t$)
            \State bestScore $\gets$ $f$(bestSolution)
            \State \Return bestChromosome, bestScore
            \EndFunction
        \end{algorithmic}
    \end{algorithm}


    \subsection{Algoritmo de comparación}

    Para la comparación de los algoritmos implementados consideraremos el algoritmo voraz \emph{Sequential forward selection}, que se puede ver en el Pseudocódigo \ref{algSFS}.

    \begin{algorithm}
        \caption{Algoritmo de comparación}\label{algSFS}
        \begin{algorithmic}[1]
            \Function{sequentialForwardSelection}{train, target}
            \State s $\gets$ genZeroSolution()
            \State bestScore $\gets$ 0
            \While{there was improvement with some feature}
                \For{every feature f in not selected features}
                    \State s $\gets$ addFeature(s,f)
                    \State currentScore $\gets$ score(s, train, target)
                    \If{currentScore $>$ bestScore}
                    \State bestScore $\gets$ currentScore
                    \State bestFeature $\gets$ f
                    \EndIf
                    \State s $\gets$ removeFeature(s,f)
                \EndFor
            \If{there was a best feature f}
            \State s $\gets$ addFeature(s,f)
            \EndIf
            \EndWhile
            \State \Return s, bestScore
            \EndFunction
        \end{algorithmic}
    \end{algorithm}

    La idea es la siguiente: en cada iteración escogemos la característica, de entre las aún no seleccionadas, que mejor valor de la función objetivo produce, si y sólo si este valor es mejor que el actual.

    \section{Desarrollo de la práctica}

    La práctica se ha desarrollado por completo en Python, definiendo cada algoritmo en una función diferente con cabeceras iguales ---mismo número y tipo de parámetros--- para poder automatizar el proceso de recogida de datos.

    \subsection{\emph{Framework} de aprendizaje automático}
    Se ha usado, además, el módulo \emph{Scikit-learn}, del que se ha usado la siguiente funcionalidad:
    \begin{itemize}
        \item Particionamiento de los datos. \emph{Scikit-learn} aporta una función para hacer un particionado aleatorio de los datos en una parte de aprendizaje y otra de test. Esto se ha usado para implementar la técnica $5\times2$ \emph{cross-validation}.
    \end{itemize}

    \subsection{Paralelización en GPU de la función objetivo}

    Aunque en la práctica anterior se usó también \emph{Scikit-learn} para medir la función objetivo, la lentitud de este proceso me llevó a buscar otras alternativas: después de intentar usar el mismo módulo con la opción de paralelización CPU y conseguir prácticamente los mismos resultados ---para notar mejoría, dicen los desarrolladores, haría falta trabajar con bases de datos con varios miles de muestras---, decidí buscar una solución propia.

    Como gracias a mi Trabajo fin de grado he aprendido a hacer computación general paralelizada en GPU, decidí usar la librería CUDA ---y en concreto su interfaz para Python, PyCUDA--- para implementar la función objetivo de una forma eficiente. La mejoría en tiempo conseguida es muy notable ---es del orden de 20 a 100 veces más rápido\footnote{Los tiempos son muy dependientes del número de muestras de la base de datos y del número de características. Para tener una idea de la mejora, se pueden comparar los tiempos de las tablas 3-NN y SFS de esta y la anterior práctica.}--- y, tras muchas pruebas para comprobar que el cálculo de la función era correcto, sustituí el $k$-NN de \emph{Scikit-learn} con el implementado en CUDA.

    Todo este trabajo, necesario para el correcto funcionamiento de la práctica, se encuentra en los ficheros bajo el directorio \emph{src/knnGPU}, que contienen la implementación en C del $k$-NN y la interfaz para poder usar el código desde Python.

    Además, como vi que este código podía beneficiar a mis compañeros, decidí publicarlo de forma abierta en un \fnurl{repositorio de Github}{https://github.com/agarciamontoro/metaheuristics}, bien documentado y con una guía de uso.

    Gracias a esto, algunos amigos me ayudaron a mejorar el código: yo había implementado sólo la función objetivo sobre los datos de training, y Jacinto Carrasco Castillo la modificó para poder hacer la medición también con los datos de test. Además, Luís Suárez Lloréns me ayudó a probar cambios que creíamos que iban a  mejorar aún más la eficiencia ---aunque tras mucho trabajo vimos que la implementación inicial era la más rápida---. Por último, Antonio Álvarez Caballero, Anabel Gómez Ríos y Gustavo Rivas Gervilla me ayudaron a testear el código, probándolo con sus algoritmos y los datos que tenían de anteriores prácticas.

    \subsection{Manual de usuario}
    Para la ejecución de la práctica es necesario tener instalado Python 3, el módulo \emph{Scikit-learn}, \emph{PyCUDA} y \emph{jinja2} ---estos dos últimos módulos son necesarios para la implementación del código paralelizado---, así como disponer de una tarjeta gráfica compatible con CUDA.

    Todo se encuentra automatizado en el fichero \emph{src/03\_multiPath.py}, así que sólo es necesario ejecutar la siguiente orden desde el directorio raíz de la práctica: \emph{python src/03\_multiPath.py}

    Así se ejecutarán todos los algoritmos con todas las bases de datos usando la ténica del  $5\times2$ \emph{cross-validation}. Las tablas generadas se guardarán en el directorio \emph{results/03}.

    La semilla utilizada se inicializa al principio de la ejecución del programa con las líneas \emph{np.random.seed(19921201)} y \emph{random.seed(19921201)}.


    \section{Análisis de resultados}

    En esta sección vamos a presentar los datos recogidos de la ejecución de todos los algoritmos con las tres bases de datos consideradas: \emph{WDBC}, \emph{Movement Libras} y \emph{Arrhytmia}. Las bases de datos se han considerado completas en todos los casos, tal y como se nos entregaron ---arreglando alguna columna defectuosa y homogeneizando el nombre de la columna de clasificación para poder automatizar el proceso---.

    Para el análisis de cada algoritmo con cada base de datos se han generado cinco particiones aleatorias de los datos y se ha ejecutado el algoritmo considerando cada partición como datos de entrenamiento y test, con la técnica \emph{$5\times2$ cross-validation}.

    En cada una de estas ejecuciones se han medido los siguientes datos:
    \begin{itemize}
        \item Tasa de clasificación en la partición de entrenamiento ---en \%---.
        \item Tasa de clasificación en la partición de test ---en \%---.
        \item Tasa de reducción de las características ---en \%---.
        \item Tiempo de ejecución ---en segundos---.
    \end{itemize}

    Veamos ya los datos y analicemos los resultados obtenidos:

    \subsection{Clasificador $k$-NN}
    \begin{table}[!htb]
        \maketable{\dataKNN}
        \caption{Datos del clasificador $k$-NN}
        \label{knn}
    \end{table}

    En la tabla \ref{knn} se pueden ver los datos obtenidos del clasificador $k$-NN. La selección de características en este algoritmo es nula, ya que es la propia función objetivo considerando la totalidad de las características. Aún así, se ha añadido aquí para conocer la tasa de clasificación en los conjuntos de entrenamiento y de test considerando como solución la trivial: esto es, todas las características.

    Como vemos, aunque en la primera base de datos las tasas de clasificación son buenas, en las otras dos son muy mejorables, lo que nos da una idea de la necesidad de la reducción de características.

    \subsection{Algoritmo de comparación}
    \begin{table}[!htb]
        \maketable{\dataSFS}
        \caption{Datos del algoritmo \emph{Sequential forward selection}}
        \label{sfs}
    \end{table}

    En la tabla \ref{sfs} vemos los resultados del algoritmo de comparación, el \emph{Sequential forward selection}. Este algoritmo voraz tiene una alta tasa de reducción de características, pero la tasa de clasificación no mejora la del clasificador con la solución trivial.

    Esto se debe a que consideramos cada característica de una forma secuencial, y una vez seleccionamos una, es imposible descartarla. Aún así, este algoritmo podría ser interesante si lo que buscamos es una reducción drástica del número de características ---como vemos, sobre el 80\%--- sin perder mucha información ---las tasas de clasificación son más o menos iguales a las del clasificador con la solución trivial---.

    \subsection{Algoritmo genético generacional}
    \begin{table}[!htb]
        \maketable{\dataAGG}
        \caption{Datos del algoritmo genético generacional con cruce clásico}
        \label{tablaAGG}
    \end{table}

    \begin{table}[!htb]
        \maketable{\dataAGGHUX}
        \caption{Datos del algoritmo genético generacional con cruce HUX}
        \label{tablaAGGHUX}
    \end{table}

    Veamos ya el primero de los dos algoritmos genéticos estudiados.

    Lo primero que podemos observar en estas dos tablas es la poca diferencia que aporta el usar uno u otro operador de cruce. No hay diferencias significativas, más allá del pequeño aumento en tiempo que tiene como consecuencia el uso del operador de cruce uniforme.

    En general, el algoritmo genético generacional consige unas tasas de clasificación incluso peores que el SFS ---sobre todo en la última base de datos---, quizás porque el número de iteraciones no es suficiente para explorar el espacio de búsqueda al completo. Por otro lado, es posible que al algoritmo genético le falte intensificación, ya que al cambiar la población entera, la diversidad está asegurada pero la intensificación en zonas concretas del espacio de soluciones no.

    Habría que estudiar qué problema tiene el algoritmo, analizando la diversidad de la población y probando alternativas, pero tal y como está ahora mismo, los resultados no son muy aceptables.

    \subsection{Algoritmo genético estacionario}
    \begin{table}[!htb]
        \maketable{\dataAGE}
        \caption{Datos del algoritmo genético estacionario con cruce clásico}
        \label{tablaAGE}
    \end{table}

    \begin{table}[!htb]
        \maketable{\dataAGEHUX}
        \caption{Datos del algoritmo genético estacionario con cruce HUX}
        \label{tablaAGEHUX}
    \end{table}

    Por último, veamos el algoritmo genético estacionario, que cambia radicalmente el esquema de reemplazamiento.

    De nuevo, podemos observar cómo la elección de uno u otro operador de cruce parece casi irrelevante. En este caso las diferencias son algo mayores, sobre todo en las tasas de reducción, pero no podemos concluir que estas diferencias sean fruto de la aleatoriedad incluida en el proceso.

    Vemos aquí también que los resultados son incluso más pobres que con el algoritmo de comparación ---con la honrosa excepción de la base de datos Libras--- donde se consiguen tasas de clasificación algo mejores.

    Estos resultados pueden deberse, igual que antes, a un límite de iteraciones demasiado bajo, que impida encontrar buenas zonas del espacio de soluciones.

    Podría ser buena idea, además, intentar intensificar sobre zonas concretas del espacio con algún tipo de búsqueda local que mejore las soluciones encontradas por los genéticos. Este estudio puede ser interesante para ver cómo influyen técnicas locales en algoritmos que buscan tanta diversidad como los genéticos.

    \subsection{Datos generales}
    \begin{table}[!htb]
        \maketablemean{\dataMedias}
        \caption{Datos generales}
        \label{medias}
    \end{table}

    Vemos aquí ya de forma mucho más clara cómo los algoritmos genéticos ---independientemente del esquema de reemplazamiento y del operador de cruce usado--- arrojan alguna mejora sobre el algoritmo de comparación, pero no tan grande como para concluir definitivamente que son mejores: en la última base de datos son claramente peores, lo que nos hace pensar que quizás debamos añadir algún comportamiento no considerado que añada intensificación.

    Este estudio debería ampliarse con más pruebas y componentes diferentes en los algoritmos para poder extraer alguna conclusión sensata.

\end{document}
